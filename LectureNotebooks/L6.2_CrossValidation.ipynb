{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6.2 The variance-bias tradeoff and cross-validation\n",
    "\n",
    "In many modeling problems, we are given the choice between models with many degrees of freedom, leading to flexibility but high complexity, or a model with few degrees of freedom but with simplicity and ease of interpretation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "options(repr.plot.width=6, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a polynomial model with iid Gaussian noise, with $N_{\\rm obs}$ data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with a polynomial. Note this is a general additive model, so we can use lm. We can compute the sum-of-squared residuals SSR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some linear data with noise\n",
    "NObs <- 10\n",
    "\n",
    "beta0 <-  6.1\n",
    "beta1 <- -0.6\n",
    "beta2 <-  0.4\n",
    "beta3 <- -0.13\n",
    "beta4 <-  0.01\n",
    "\n",
    "sigma <- 0.8\n",
    "\n",
    "X <- seq(0,10,length=NObs)\n",
    "eps <- rnorm(NObs,0,sigma)\n",
    "\n",
    "YBar <- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + beta4*X^4\n",
    "\n",
    "Y <- YBar + eps\n",
    "\n",
    "plot(X,Y)\n",
    "lines(X,YBar, col='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel <- lm(Y~ X + I(X^2 ) + I(X^3 ) + I(X^4 ))\n",
    "\n",
    "fitModel\n",
    "\n",
    "SSR <- sum((predict(fitModel) - mean(Y))^2)\n",
    "\n",
    "SSR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for various degrees of the polynomial, $N_{\\rm poly}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SSRArray <-0\n",
    "\n",
    "fitModel <- lm(Y~ X )\n",
    "SSRArray[1] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) )\n",
    "SSRArray[2] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) )\n",
    "SSRArray[3] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) )\n",
    "SSRArray[4] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) )\n",
    "SSRArray[5] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) )\n",
    "SSRArray[6] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) )\n",
    "SSRArray[7] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) )\n",
    "SSRArray[8] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) )\n",
    "SSRArray[9] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "plot(SSRArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An easier way to do lm for many different polynomials is to use the poly function in a loop...\n",
    "\n",
    "SSRArray <-0\n",
    "\n",
    "for (iPolyDegree in 1:9){\n",
    "\n",
    "    fitModel <- lm(Y~ poly(X, iPolyDegree) )\n",
    "    SSRArray[iPolyDegree] <- sum((predict(fitModel) - Y)^2)\n",
    "\n",
    "}\n",
    "    \n",
    "plot(SSRArray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of fit\n",
    "\n",
    "Define a __goodness of fit__ with the noise-free true error \n",
    "$$\\sum \\left( \\bar{y}_{\\rm true}(x_i) - \\bar{y}_{\\rm fit}(x_i)\\right)^2$$\n",
    "Note this goodness of fit is different than the sum-of-squared residuals\n",
    "$$SSR = \\sum \\left( y(x_i) - \\bar{y}_{\\rm fit}(x_i)\\right)^2,$$\n",
    "for example, we can compute the SSR from data, whereas the noise-free true error can only be computed if we know the true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoFArray <-0\n",
    "\n",
    "fitModel <- lm(Y~ X )\n",
    "GoFArray[1] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) )\n",
    "GoFArray[2] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) )\n",
    "GoFArray[3] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) )\n",
    "GoFArray[4] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) )\n",
    "GoFArray[5] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) )\n",
    "GoFArray[6] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) )\n",
    "GoFArray[7] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) )\n",
    "GoFArray[8] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "fitModel <- lm(Y~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) )\n",
    "GoFArray[9] <- sum((predict(fitModel) - YBar)^2)\n",
    "\n",
    "plot(log(GoFArray))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "How can we define goodness of fit when we do not have the ability to simulate?\n",
    "\n",
    "Leave-one-out (LOOCV). The remaining data are called the __training set__ while the left-out data is the __validation set__. Repeat this $N_{\\rm obs}$ times and compute the goodness of fit for all these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoF <-0 # Variable to be used for average Goodness of Fit across iLeaveMeOut's\n",
    "\n",
    "for (iLeaveMeOut in 1:NObs){\n",
    "    \n",
    "    XTraining <- X[-iLeaveMeOut] # everything but the one we leave out\n",
    "    YTraining <- Y[-iLeaveMeOut]\n",
    "    \n",
    "    XValidation <- X[iLeaveMeOut] # the one we leave out\n",
    "    YValidation <- Y[iLeaveMeOut]\n",
    "    \n",
    "    fitModel <- lm(YTraining ~ XTraining )\n",
    "        \n",
    "    GoF <- GoF + (predict(fitModel,data.frame(XTraining = XValidation))-YValidation)^2\n",
    "\n",
    "}\n",
    "\n",
    "GoF <- GoF/NObs\n",
    "\n",
    "GoF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GoFArray <- 0\n",
    "\n",
    "for (iPolyDegree in 1:8){\n",
    "\n",
    "    \n",
    "    GoF <- 0 # Variable to be used for average Goodness of Fit across iLeaveMeOut's\n",
    "\n",
    "    for (iLeaveMeOut in 1:NObs){\n",
    "    \n",
    "        XTraining <- X[-iLeaveMeOut]\n",
    "        YTraining <- Y[-iLeaveMeOut]\n",
    "    \n",
    "        XValidation <- X[iLeaveMeOut]\n",
    "        YValidation <- Y[iLeaveMeOut]\n",
    "    \n",
    "        fitModel <- lm(YTraining ~ poly(XTraining,iPolyDegree) )\n",
    "        \n",
    "        # Compute the average GoF across the iLeaveMeOut's\n",
    "        GoF <- GoF + (predict(fitModel,data.frame(XTraining = XValidation))-YValidation)^2\n",
    "    }\n",
    "\n",
    "    GoF <- GoF/NObs # Compute the average GoF across the iLeaveMeOut's\n",
    "\n",
    "    GoFArray[iPolyDegree] <- GoF\n",
    "\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(log(GoFArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the \"true error\" example at the top of this notebook, we went to a ninth-degree polynomial. In the LOOCV example, we only went to an eighth-degree polynomial. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoF <-0 # Variable to be used for average Goodness of Fit across iLeaveMeOut's\n",
    "\n",
    "for (iLeaveMeOut in 1:NObs){\n",
    "    \n",
    "    XTraining <- X[-iLeaveMeOut] # everything but the one we leave out\n",
    "    YTraining <- Y[-iLeaveMeOut]\n",
    "    \n",
    "    XValidation <- X[iLeaveMeOut] # the one we leave out\n",
    "    YValidation <- Y[iLeaveMeOut]\n",
    "    \n",
    "    fitModel <- lm(YTraining ~ poly(XTraining,9) )\n",
    "        \n",
    "    GoF <- GoF + (predict(fitModel,data.frame(XTraining = XValidation))-YValidation)^2\n",
    "\n",
    "}\n",
    "\n",
    "GoF <- GoF/NObs\n",
    "\n",
    "GoF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, remove k-fold (10fCV). The remaining data are called the __training set__ while the left-out data is the __testing set__. Repeat this $N_{\\rm obs}$ times and compute the goodness of fit for all these. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
